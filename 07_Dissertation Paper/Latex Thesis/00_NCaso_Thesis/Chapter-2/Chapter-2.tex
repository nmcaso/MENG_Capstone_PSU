\chapter{Background}
\label{chapter2:background}
\graphicspath{{Chapter-2/graphics/}}

\section{Delay and Sum Overview}
\label{chapter2:das_Overview}

    The simplest derivation of Delay-and-Sum beamforming is described with a 1-Dimensional line array of transducers. This derivation loosely follows that in Perrot et. al, 2021\cite{perrotYouThinkYou2021a}, but dates back to the 1930s and phased array radio communication technology\cite{1686435}. Let an incident plane-wave intersect with the transducer array at an angle $\theta$ relative to the array. If every transducer records a time series signal, the incident wave appears at different times across the transducer array. To determine the incident angle $\theta$, one only needs two transducers, the distance between them, $L_x$, and the wave speed $c$. Suppose the time delays from each transducer are $t_1$ and $t_2$:

    \begin{equation} 
        \theta = cos^{-1}(\frac{t_2 - t_1}{c L_x})
    \end{equation}

    This can identify the incident angle from a single relative time-delay, but what if there are several incident waves? In this case, it is better to compare the spatially constructive signal amplitudes in several locations or directions. This is done by solving the above equation for the relative delay $t_2 - t_1$ for many angles $\theta_i$, and taking the signal amplitude at those delays:

    \begin{equation}
        t_{delay,i} = \frac{L_x}{c} cos(\theta_i)
    \end{equation}

    This equation is in polar coordinates in terms of $\theta_i$. A cartesian form may also be derived to solve for $t_{delay,i}$ using trigonometry and given a set of virtual points $(x_i,z)$ to calculate the signal amplitude at, assuming a constant depth $z$:

    \begin{equation} \label{eq:timedelay}
        t_{delay,i} = \frac{1}{c} \sqrt{(x_i - x_0)^2 + z^2}
    \end{equation}

    We have additionally accounted for the distance between transducer elements in the positions of $x_i$, that is, $\Delta{x} = x_{i+1} - x_i$.

    Next, the contribution from one transducer to the final set of amplitudes comes from delaying that transducer's signal by the set of delays we calculated $t_{delay,i}$, (the "delay" in "delay and sum"). Repeat this process for each transducer, adding the signal amplitudes (the "sum" from "delay and sum") from each transducer together, to create the final image. For a 1-dimensional array and imaging area, the 'image' is a 1-dimensional line.

\section{DAS with a Delay Matrix}
\label{chapter2:delay_matrix}

    The time delays from equation \ref{eq:timedelay} can be organized into a "delay matrix", so as to coalesce data for optimal performance. The Delay-Matrix matrix is n+1-dimensional for an n-dimensional array of transducers, and contains a map of the relative delays between each pixel's virtual location and each transducer's location. It uses the distance formula at every point in the list and at every transducer location. For a spatial vector $\vv{r}$ containing the coordinates of the desired pixel locations and a spatial vector $\vv{x}$ containing transducer coordinates, and for the $k^{th}$ transducer: 

    \begin{equation}
        M_{\vv{r},k}(t) = \frac{1}{c} || \vv{r_k} - \vv{x_k}||_2
    \end{equation}

    Where $||\cdot||$ denotes the $l_2$ norm, or distance formula. To beamform the array of signals in a discrete system, the time-delays must be translated into delay indices. For a digital system with sampling rate $f_s$, time increment $\Delta{t} = \frac{1}{f_s}$:

    \begin{equation}
        M_{\vv{r},k}(i) = \frac{1}{\Delta{t}} \times M_{\vv{r},k}(t)    
    \end{equation}

    The delay matrix can efficiently delay time signals by indexing, because it coalesces indices to adjacent memory locations. The image resultant from an n-dimensional array of transducers is a n-dimensional array of beamformed signals that represent the amplitude contributions of each transducer to the virtual locations from list of pixel points, $\vv{r}$.

    Indexing into a delay matrix requires integer delays for discrete systems \cite{burger2013principles}. This implies that some fraction between indices is lost. Assuming a linear, smooth timeseries, one can perform interpolation between the indices. The easiest method is linear interpolation by means of two delay matrices with adjacent indices, and a weighting matrix with the fractional part of the delay matrix \cite{perrotYouThinkYou2021a}.

\section{DAS by Matrix Multiplication}
\label{chapter2:mmult_das}

    Delay and Sum by its nature contains a delaying by indices and subsequent summation, which fits the definition of matrix multiplication if the matrix index positions represent delays\cite{6841027}. In this formulation, matrix rows represent pixels, and matrix columns represent the indices of the transducers delayed at a pixel's location. The number of rows is equal to the number of samples in a single frame of data multiplied by the number of transducers. We multiply this matrix by a vector of the time data from every transducer in the same order as the transducers are arrayed in the matrix:

    \begin{equation}
        \vv{b}_{i \times j} = M_{i\times j, k}\vv{x}_{k}
    \end{equation}

    Where $\vv{b}$ is a vector containing the beamformed image of size $\vv{b} \in \{1 \times n_{pixels}\}$ The image in $x$ and $y$ pixels is formed by reshaping vector $\vv{b}$ by the pixel dimensions to obtain $B_{ij}$. Because we use the matrix' indices to represent time delays for single points in time, the matrix M is $s$-sparse, where $s = n_{pixels}\times n_{transducers}$. Sparse matrix multiplication has many well-documented fast algorithms, and lends itself to very fast computations\cite{gilbertSparseMatricesMATLAB1992}.

\section{DAS for Large Images}
\label{chapter2:large_images}

    DAS by matrix multiplication and by index matrix are fast and coalesced, but at the cost of memory. Specifically, for both methods, the memory required is $n_{pixels}\times n_{transducers} \times sizeof(datatype)$, where the size of the datatype is 4 bytes for $single$ precision floating point numbers, and 8 for $double$ precision. Increasing the image dimensions eventually creates a delay index matrix or sparse matrix larger than a computer's available random-access memory (RAM). Typically the next step is to allocate memory to a hard disk drive (HDD) or solid-state drive (SSD). Both are slow and disproportinately curtail computational speed.

    For the case of large images, faster computation is achieved by generating the index matrix for each transducer, performing delay, and then overwriting the used delay matrix with the next transducer's index matrix. We can do this by calculating the index matrix and overwriting the previous transducer's information, thus removing the need to store data for all transducers. However, the computational speed is low, as this method relies on loops and many calculations that are separated out if using a delay matrix.

    A novel method presented to address this issue is generating the delay-matrix by translation or rotation. This method shifts the perspective from a stationary 'bird's-eye' view of calculating the index matrix point by point to one where we calculate a delay-matrix once and simply change our frame of reference on that delay matrix for different transducers by translating and rotating as if the delay matrix were an image. The specifics of this algorithm are introduced in \ref{chapter3:Large_Images}.

\section{GPU Acceleration}
\label{chapter2:gpu_accel}

    Graphics Processing Units (GPUs) are a broadly applicable method of parallel processing for many processes and fields. A Central Processing Unit (CPU) has comparatively fewer, but low-latency individual cores that are best for processing objects in series (serial processing), whereas a GPU contains many, individually slower cores that are best for processing objects in parallel \cite{caulfieldCPUVsGPU2009}. While a CPU's cores and larger cache memory allow it to adapt faster than a GPU core to process many different different problems in sequence, the sheer number of cores in a GPU lends the advantage when performing identical calculations on a fewer, but much larger objects.

    High performance CPUs and GPUs are ubiquitous in modern laptop and desktop computers today; however, serial programming on CPUs is far more common than parallel programming for GPUs. This is in part because of the need for an application program interface (API) between a CPU host and GPU device. Starting to program with a GPU API involves a learning curve, and GPU manufacturers typically have proprietary APIs that are not cross-compatible. For example, Compute Unified Device Architecture (CUDA) by NVIDIA \cite{CUDAZoneLibrary2017}, and the Heterogeneous Interface for Portability (HIP) by AMD Radeon \cite{IntroductionHIPFAQ}. OpenCL is a recent movement to support a universal GPU language that interfaces with all of the proprietary APIs depending on a machine's specific hardware \cite{OpenCLOpenStandard2013}. This paper, however, performs parallel computing using NVIDIA's language, CUDA, based on the hardware available for the reasearch.

    GPU programming is single instruction, multiple data (SIMD), which means it applies a single instruction across an array of some or all of the GPU cores \cite{cardosoChapterHighperformanceEmbedded2017}. Therefore, the code is generally written in vector format, similar to MATLAB (except MATLAB's instructions are only notatinally vectorized and it uses the CPU) \cite{VectorizationMATLABSimulink}.

    Delay and sum may be anatomized as a small series of operations on a large set of data as described in section \ref{chapter2:delay_matrix}. Therefore DAS on a GPU has high potential for speedup. Chapter \ref{chapter3:Methodology} characterizes the performance improvements between Delay and Sum on a CPU versus on a GPU and the aforementioned methods in this chapter.